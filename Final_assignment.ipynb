{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INITIALIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy for math calculations\n",
    "import numpy as np\n",
    "def dummy_npwarn_decorator_factory():\n",
    "  def npwarn_decorator(x):\n",
    "    return x\n",
    "  return npwarn_decorator\n",
    "np._no_nep50_warning = getattr(np, '_no_nep50_warning', dummy_npwarn_decorator_factory)\n",
    "\n",
    "# import pandas for data (csv) manipulation\n",
    "import pandas as pd\n",
    "\n",
    "# import gc to collect garbage\n",
    "import gc\n",
    "\n",
    "# import matplotlib for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.style.use('fivethirtyeight') \n",
    "%matplotlib inline\n",
    "\n",
    "# import seaborn for more plotting options(built on top of matplotlib)\n",
    "import seaborn as sns\n",
    "\n",
    "# import librosa for analysing audio signals : visualize audio, display the spectogram\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "\n",
    "# import librosa for analysing audio signals : visualize audio, display the spectogram\n",
    "import librosa.display\n",
    "\n",
    "# import wav for reading and writing wav files\n",
    "import wave\n",
    "\n",
    "# import IPython.dispaly for playing audio in Jupter notebook\n",
    "import IPython.display as ipd\n",
    "\n",
    "# import os for system operations\n",
    "import os\n",
    "\n",
    "# import random for get random values/choices\n",
    "import random\n",
    "\n",
    "from scipy.stats import skew\n",
    "from scipy.stats import kurtosis\n",
    "\n",
    "# importing Machine Learning Models\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# importing from sklearn the evaluation metrics for classification\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# importing from sklearn model selection \n",
    "from sklearn.model_selection import PredefinedSplit, GridSearchCV, train_test_split, cross_val_score, StratifiedKFold, learning_curve\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sklearn.impute import SimpleImputer, MissingIndicator\n",
    "\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline, make_pipeline\n",
    "\n",
    "# import tqdm to show a smart progress meter\n",
    "from tqdm.notebook import trange,tqdm\n",
    "\n",
    "# import warnings to hide the unnessairy warniings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=42): \n",
    "    random.seed(seed) \n",
    "    os.environ['PYTHONHASHSEED'] = str(seed) \n",
    "    np.random.seed(seed) \n",
    "SEED = 42\n",
    "seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CONFIGURATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get a list of the pathes of all the audio files\n",
    "INPUT_DIR = \"./ESC-50/input/audio/\"\n",
    "INPUT_FEATURE_FILE = \"./ESC-50/input/Esc50_features_extracted.csv\"\n",
    "AUG_DIR = \"./ESC-50/augmented_input/audio/\"\n",
    "AUG_FEATURE_FILE = \"./ESC-50/augmented_input/Esc50_features_extracted.csv\"\n",
    "\n",
    "#\n",
    "AUGMENT_DATA = False\n",
    "        \n",
    "# A dictionary to decode the categories into targets\n",
    "DECODER = {0: 'dog', 14: 'chirping_birds', 36: 'vacuum_cleaner', 19: 'thunderstorm', 30: 'door_wood_knock',34: 'can_opening', 9: 'crow', 22: 'clapping', 48: 'fireworks', 41: 'chainsaw', 47: 'airplane', 31: 'mouse_click', 17: 'pouring_water', 45: 'train', 8: 'sheep', 15: 'water_drops', 46: 'church_bells', 37: 'clock_alarm', 32: 'keyboard_typing', 16: 'wind', 25: 'footsteps', 4: 'frog', 3: 'cow', 27: 'brushing_teeth', 43: 'car_horn', 12: 'crackling_fire', 40: 'helicopter', 29: 'drinking_sipping', 10: 'rain', 7: 'insects', 26: 'laughing', 6: 'hen', 44: 'engine', 23: 'breathing', 20: 'crying_baby', 49: 'hand_saw', 24: 'coughing', 39: 'glass_breaking', 28: 'snoring', 18: 'toilet_flush', 2: 'pig', 35: 'washing_machine', 38: 'clock_tick', 21: 'sneezing', 1: 'rooster', 11: 'sea_waves', 42: 'siren', 5: 'cat', 33: 'door_wood_creaks', 13: 'crickets'}\n",
    "\n",
    "# A dictionary to encode the categories into targets\n",
    "ENCODER = {'dog': 0, 'chirping_birds': 14, 'vacuum_cleaner': 36, 'thunderstorm': 19, 'door_wood_knock': 30, 'can_opening': 34, 'crow': 9, 'clapping': 22, 'fireworks': 48, 'chainsaw': 41, 'airplane': 47, 'mouse_click': 31, 'pouring_water': 17, 'train': 45, 'sheep': 8, 'water_drops': 15, 'church_bells': 46, 'clock_alarm': 37, 'keyboard_typing': 32, 'wind': 16, 'footsteps': 25, 'frog': 4, 'cow': 3, 'brushing_teeth': 27, 'car_horn': 43, 'crackling_fire': 12, 'helicopter': 40, 'drinking_sipping': 29, 'rain': 10, 'insects': 7, 'laughing': 26, 'hen': 6, 'engine': 44, 'breathing': 23, 'crying_baby': 20, 'hand_saw': 49, 'coughing': 24, 'glass_breaking': 39, 'snoring': 28, 'toilet_flush': 18, 'pig': 2, 'washing_machine': 35, 'clock_tick': 38, 'sneezing': 21, 'rooster': 1, 'sea_waves': 11, 'siren': 42, 'cat': 5, 'door_wood_creaks': 33, 'crickets': 13}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA AUGMENTATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_noise(data):\n",
    "    noise = np.random.normal(0, 0.1, len(data))\n",
    "    audio_noisy = data + noise\n",
    "    return audio_noisy\n",
    "    \n",
    "def pitch_shifting(data):\n",
    "    sr  = 16000\n",
    "    bins_per_octave = 12\n",
    "    pitch_pm = 2\n",
    "    pitch_change =  pitch_pm * 2*(np.random.uniform())   \n",
    "    data = librosa.effects.pitch_shift(y = data.astype('float64'),  sr = sr, n_steps=pitch_change, bins_per_octave=bins_per_octave)\n",
    "    return data\n",
    "\n",
    "def random_shift(data):\n",
    "    timeshift_fac = 0.2 *2*(np.random.uniform()-0.5)  # up to 20% of length\n",
    "    start = int(data.shape[0] * timeshift_fac)\n",
    "    if (start > 0):\n",
    "        data = np.pad(data,(start,0),mode='constant')[0:data.shape[0]]\n",
    "    else:\n",
    "        data = np.pad(data,(0,-start),mode='constant')[0:data.shape[0]]\n",
    "    return data\n",
    "\n",
    "def volume_scaling(data):\n",
    "    sr  = 16000\n",
    "    dyn_change = np.random.uniform(low=1.5,high=2.5)\n",
    "    data = data * dyn_change\n",
    "    return data\n",
    "    \n",
    "def time_stretching(data, rate=1.5):\n",
    "    input_length = len(data)\n",
    "    streching = data.copy()\n",
    "    streching = librosa.effects.time_stretch(y = streching, rate = rate)\n",
    "    \n",
    "    if len(streching) > input_length:\n",
    "        streching = streching[:input_length]\n",
    "    else:\n",
    "        streching = np.pad(data, (0, max(0, input_length - len(streching))), \"constant\")\n",
    "    return streching\n",
    "def save_augmentation(filepath, aug):\n",
    "    aug = np.array(aug,dtype='float32').reshape(-1,1)\n",
    "    sf.write(filepath, aug, 16000, 'PCM_24')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_aug(input_dir, aug_dir):\n",
    "    if not os.path.exists(aug_dir):\n",
    "      os.makedirs(aug_dir)\n",
    "      path_ = np.random.choice(os.listdir(input_dir), size = (2000,), replace= False)\n",
    "      for k,files in zip(trange(len(path_)), path_):\n",
    "          data_, fs = librosa.load(os.path.join(input_dir, files), sr = 16000)\n",
    "          noise_data = add_noise(data_)\n",
    "          # pitch_data = pitch_shifting(data_)\n",
    "          random_shift_data = random_shift(data_)\n",
    "          volume_scale_data = volume_scaling(data_)\n",
    "          time_stretching_data =  time_stretching(data_, rate=1.5)\n",
    "          aug = [noise_data,time_stretching_data, random_shift_data,volume_scale_data ]\n",
    "          for j in range(len(aug)):\n",
    "            filepath = os.path.join(aug_dir, files[0:2]+'generated'+'-'+str(j)+'-'+str(k)+'-'+files[2:])\n",
    "            save_augmentation(filepath, aug[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "if AUGMENT_DATA:\n",
    "  data_aug(INPUT_DIR, AUG_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FEATURE EXTRACTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_from_files(dir):\n",
    "    cols = ['filename','fold','target','files_path']\n",
    "    data = []\n",
    "    for path, subdirs, files in os.walk(dir):\n",
    "        for name in files:\n",
    "            fold = int(name[0])\n",
    "            target = int(name.split('-')[-1].replace('.wav', ''))\n",
    "            file_path = dir + name\n",
    "            data.append((name, fold, target, file_path))\n",
    "    dataset = pd.DataFrame(data, index = range(len(data)), columns = cols)\n",
    "    return(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_stats(values):\n",
    "    stats = []\n",
    "    stats.extend(np.mean(values, axis=1))\n",
    "    stats.extend(np.std(values, axis=1))\n",
    "    stats.extend(skew(values, axis=1))\n",
    "    stats.extend(kurtosis(values, axis=1))\n",
    "    stats.extend(np.median(values, axis=1))\n",
    "    stats.extend(np.min(values, axis=1))\n",
    "    stats.extend(np.max(values, axis=1))\n",
    "    return(stats)\n",
    "\n",
    "def extract_features_from_audio_file(audio_path):\n",
    "    y , sr = librosa.load(audio_path, mono=True)\n",
    "\n",
    "    zcr = librosa.feature.zero_crossing_rate(y)\n",
    "    cqt = np.abs(librosa.cqt(y, sr=sr, tuning=None))\n",
    "\n",
    "    chroma_cqt = librosa.feature.chroma_cqt(C=cqt, n_chroma=12)\n",
    "    chroma_cens = librosa.feature.chroma_cens(C=cqt, n_chroma=12)\n",
    "    tonnetz = librosa.feature.tonnetz(chroma=chroma_cens)\n",
    "\n",
    "    del cqt\n",
    "    S, phase = librosa.magphase(librosa.stft(y))\n",
    "    power_S = S**2\n",
    "    del y\n",
    "\n",
    "    chroma_stft = librosa.feature.chroma_stft(S=power_S, n_chroma=12)\n",
    "\n",
    "    rmse = librosa.feature.rms(S=S)\n",
    "\n",
    "    spectral_centroid = librosa.feature.spectral_centroid(S=S)\n",
    "    spectral_bandwidth = librosa.feature.spectral_bandwidth(S=S)\n",
    "    spectral_contrast = librosa.feature.spectral_contrast(S=S, n_bands=6)\n",
    "    spectral_rolloff = librosa.feature.spectral_rolloff(S=S)\n",
    "    spectral_flatness = librosa.feature.spectral_flatness(S=S)\n",
    "\n",
    "    mel = librosa.feature.melspectrogram(sr=sr, S=power_S)\n",
    "    del S, power_S\n",
    "    mfcc = librosa.feature.mfcc(S=librosa.power_to_db(mel), n_mfcc=20)\n",
    "\n",
    "    features = [chroma_stft,chroma_cqt,chroma_cens,tonnetz,mfcc,rmse,zcr,spectral_centroid,spectral_bandwidth,spectral_contrast,spectral_rolloff,spectral_flatness]\n",
    "    stats = []\n",
    "    for val in features:\n",
    "        stats.extend(feature_stats(val))\n",
    "    return(stats)\n",
    "\n",
    "\n",
    "def extract_features_from_dir(audio_dir):\n",
    "    dataset = get_dataset_from_files(audio_dir)\n",
    "    \n",
    "    stats = ['mean','std','skew','kurtosis','median','min','max']\n",
    "    cols = []\n",
    "    data = []\n",
    "    \n",
    "    feature_sizes = {'chroma_stft':12, 'chroma_cqt':12, 'chroma_cens':12,\n",
    "                         'tonnetz':6, 'mfcc':20, 'rmse':1, 'zcr':1,\n",
    "                         'spectral_centroid':1, 'spectral_bandwidth':1,\n",
    "                         'spectral_contrast':7, 'spectral_rolloff':1,\n",
    "                         'spectral_flatness':1\n",
    "                         }\n",
    "\n",
    "    def generate_columns(name, values):\n",
    "        for stat in stats:\n",
    "            for i in range(values):\n",
    "              column = stat + '_' + name\n",
    "              if values > 1 :\n",
    "                 column = column + f'_{i}'\n",
    "              cols.append(column)\n",
    "\n",
    "    for key, value in feature_sizes.items():\n",
    "       generate_columns(key, value)\n",
    "\n",
    "    n_samples = dataset['files_path'].shape[0]\n",
    "    for i in trange(n_samples):\n",
    "      data.append(extract_features_from_audio_file(dataset['files_path'][i]))\n",
    "\n",
    "    feature_set = pd.DataFrame(data, index = range(len(data)), columns = cols)\n",
    "\n",
    "    return(pd.concat([dataset,feature_set], axis=1))\n",
    "\n",
    "def get_features_dataset(audio_dir,feature_file):\n",
    "    if os.path.exists(feature_file):\n",
    "      dataset = pd.read_csv(feature_file)\n",
    "    else:\n",
    "      dataset = extract_features_from_dir(audio_dir)\n",
    "      dataset.to_csv(feature_file, index=False)\n",
    "    return(dataset)\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_dataset = get_features_dataset(INPUT_DIR,INPUT_FEATURE_FILE)\n",
    "if AUGMENT_DATA:\n",
    "  augmented_feature_dataset = get_features_dataset(AUG_DIR,AUG_FEATURE_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA PREPARATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(dataset):\n",
    "\n",
    "  learn = dataset[dataset['fold'] < 5]\n",
    "  validate = feature_dataset[feature_dataset['fold'] == 5]\n",
    "\n",
    "  X_validate = validate.drop(columns=['fold','filename','target','files_path'])\n",
    "  y_validate = validate.target\n",
    "\n",
    "  ps = PredefinedSplit(learn.fold)\n",
    "  X = learn.drop(columns=['fold','filename','target','files_path'])\n",
    "  y = learn.target\n",
    "\n",
    "  return X, y, ps, X_validate, y_validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "if AUGMENT_DATA:\n",
    "  X, y, ps, X_validate, y_validate = split_data(augmented_feature_dataset)\n",
    "else:\n",
    "  X, y, ps, X_validate, y_validate = split_data(feature_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRID SEARCH CROSS-VALIDATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyGridSearchCV:\n",
    "    \n",
    "  def __init__(self,type,param_grid):\n",
    "    self.type=type\n",
    "\n",
    "    self.scaler = MinMaxScaler()\n",
    "\n",
    "    self.filler_list=[\n",
    "        ('features', SimpleImputer(strategy='mean')),\n",
    "        ('indicators', MissingIndicator(features=\"all\"))]\n",
    "    \n",
    "    self.filler = FeatureUnion(transformer_list=self.filler_list)\n",
    "\n",
    "    self.estimator = self.get_estimator()\n",
    "\n",
    "    self.grid_search = GridSearchCV(estimator=self.estimator,\n",
    "                              param_grid=param_grid,\n",
    "                              #  n_jobs=-1,\n",
    "                              cv=ps,\n",
    "                              scoring='accuracy',\n",
    "                              verbose=3)\n",
    "    \n",
    "\n",
    "    self.gridsearch_pipeline_steps=[  ('scaling'  , self.scaler),\n",
    "                                      ('filler'   , self.filler),\n",
    "                                      ('gridsearch', self.grid_search)]\n",
    "\n",
    "    self.gridsearch_pipeline = Pipeline(steps=self.gridsearch_pipeline_steps, verbose=True)\n",
    "  \n",
    "  def get_estimator(self):\n",
    "    if self.type == 'LinearSVC':\n",
    "      return LinearSVC(random_state=SEED)\n",
    "    elif self.type == 'SVC':\n",
    "      return SVC(random_state=SEED)\n",
    "    elif self.type == 'KNeighborsClassifier':\n",
    "      return KNeighborsClassifier()\n",
    "    elif self.type == 'RandomForestClassifier':\n",
    "      return RandomForestClassifier(random_state=SEED)\n",
    "\n",
    "  def tune_hyperparameters(self,X,y):\n",
    "    self.gridsearch_pipeline.fit(X, y)\n",
    "    print(\n",
    "      \"The best parameters are %s with a score of %0.2f\"\n",
    "      % (self.gridsearch_pipeline.named_steps['gridsearch'].best_params_, self.gridsearch_pipeline.named_steps['gridsearch'].best_score_)\n",
    "    )\n",
    "\n",
    "    self.best_estimator_=self.gridsearch_pipeline.named_steps['gridsearch'].best_estimator_\n",
    "    self.best_pipeline_steps=[  ('scaling'  , self.scaler),\n",
    "                                ('filler'   , self.filler),\n",
    "                                ('estimator', self.best_estimator_)]\n",
    "\n",
    "    self.best_pipeline = Pipeline(steps=self.best_pipeline_steps, verbose=True)\n",
    "\n",
    "    y_predicted = self.predict(X)\n",
    "    self.accuracy_train = accuracy_score(y,y_predicted)\n",
    "    self.f1_score_train = f1_score(y,y_predicted,average='macro')\n",
    "    self.precision_score_train = precision_score(y,y_predicted,average='macro')\n",
    "    self.recall_score_train = recall_score(y,y_predicted,average='macro')\n",
    "\n",
    "  def predict(self,X):\n",
    "    y_predicted = self.best_pipeline.predict(X)\n",
    "    return(y_predicted)\n",
    "\n",
    "  def validate(self,X,y):\n",
    "    y_predicted = self.predict(X)\n",
    "    self.accuracy_validate = accuracy_score(y,y_predicted)\n",
    "    self.f1_score_validate = f1_score(y,y_predicted,average='macro')\n",
    "    self.precision_score_validate = precision_score(y,y_predicted,average='macro')\n",
    "    self.recall_score_validate = recall_score(y,y_predicted,average='macro')\n",
    "    cols = ['dataset', 'accuracy', 'f1_score', 'precision', 'recall']\n",
    "    data = []\n",
    "    data.append(('Training+Testing', self.accuracy_train, self.f1_score_train, self.precision_score_train, self.recall_score_train))\n",
    "    data.append(('Validation', self.accuracy_validate, self.f1_score_validate, self.precision_score_validate, self.recall_score_validate))\n",
    "    report = pd.DataFrame(data, index = None, columns = cols)\n",
    "    return(report.style.background_gradient(cmap= plt.cm.Blues))\n",
    "  \n",
    "  \n",
    "  def plot_confusion_matrix(self, X, y, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    self.cm = confusion_matrix(y, self.predict(X))\n",
    "    plt.imshow(self.cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "  def label(self, X):\n",
    "    return(self.decode(self.predict(X)))\n",
    "\n",
    "  def decode(self, labels):\n",
    "    decoded = []\n",
    "    for label in labels:\n",
    "      decoded.append(DECODER[label])\n",
    "    return decoded\n",
    "  \n",
    "  def encode(self, labels):\n",
    "    encoded = []\n",
    "    for label in labels:\n",
    "      encoded.append(ENCODER[label])\n",
    "    return encoded\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "    # 'C': [0.1*2**(-3), 0.1*2**(-2), 0.1*2**(-1), 0.1*2**0, 0.1*2**1, 0.1*2**2, 0.1*2**3]\n",
    "    # 'C': [0.001, 0.01, 0.1, 1, 10],\n",
    "    # 'C': [0.01, 0.1, 1]\n",
    "    # 'penalty': ['l1', 'l2'],\n",
    "    # 'loss': ['hinge', 'squared_hinge'],\n",
    "    # 'multi_class': ['ovr', 'crammer_singer']\n",
    "    # 'tol': [1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1]\n",
    "}\n",
    "best_param_grid = {\n",
    "    'C': [0.1]\n",
    "}\n",
    "# grid_linearsvc = MyGridSearchCV('LinearSVC',param_grid)\n",
    "grid_linearsvc = MyGridSearchCV('LinearSVC',best_param_grid)\n",
    "grid_linearsvc.tune_hyperparameters(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_linearsvc.gridsearch_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_linearsvc.validate(X_validate,y_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_linearsvc.plot_confusion_matrix(X_validate, y_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_linearsvc.label(X_validate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    # 'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000],\n",
    "    'C': [0.001, 0.1, 10],\n",
    "    'gamma': [1, 10, 100, 1000],\n",
    "    # 'kernel': ['rbf','linear','poly']\n",
    "    'kernel': ['linear','poly']\n",
    "}\n",
    "grid_svc = MyGridSearchCV('SVC',param_grid)\n",
    "grid_svc.tune_hyperparameters(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_svc.validate(X_validate,y_validate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KNeighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid ={\n",
    "  'n_neighbors'  :  [1,10, 1],\n",
    "  'leaf_size'    :  [20,40,1],\n",
    "  'p'            :  [1,2],\n",
    "  'weights'      :  ['uniform', 'distance'],\n",
    "  'metric'       :  ['minkowski', 'chebyshev']\n",
    "}\n",
    "grid_kneighbors = MyGridSearchCV('KNeighborsClassifier',param_grid)\n",
    "grid_kneighbors.tune_hyperparameters(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_kneighbors.validate(X_validate,y_validate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = { \n",
    "    # 'n_estimators': [25, 50, 100, 150], \n",
    "    'n_estimators': [50], \n",
    "    'max_depth': [3, 6, 9], \n",
    "    'min_samples_split': [2, 5, 10], \n",
    "    # 'min_samples_leaf': [1, 5], \n",
    "    'min_samples_leaf': [1], \n",
    "    # 'max_features': ['sqrt', 'log2', None], \n",
    "    'max_features': ['log2'], \n",
    "    'max_leaf_nodes': [3, 6, 9], \n",
    "    # 'max_leaf_nodes': [None], \n",
    "} \n",
    "\n",
    "grid_rfc = MyGridSearchCV('RandomForestClassifier',param_grid)\n",
    "grid_rfc.tune_hyperparameters(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_rfc.validate(X_validate,y_validate)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ESC50",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
